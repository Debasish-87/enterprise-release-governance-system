name: Intelligent Testing Pipeline

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["main"]
  workflow_dispatch:

permissions:
  contents: write

jobs:



  # ============================================================
  #  LAYER 1 — Application Testing + Allure Report
  # ============================================================
  application-testing:
    name: Layer 1 - Application Testing + Allure
    runs-on: ubuntu-latest
    timeout-minutes: 25

    steps:
      - name: Checkout Testing Platform Repo
        uses: actions/checkout@v4

      - name: Stage 1 - Validate Repository Structure
        run: |
          echo "===== Repo Root Files ====="
          ls -la

          echo "===== Checking required folders ====="
          test -d application-testing || (echo "❌ application-testing folder missing" && exit 1)
          test -d security-testing || (echo "❌ security-testing folder missing" && exit 1)
          test -d sbom-testing || (echo "❌ sbom-testing folder missing" && exit 1)
          test -d kpqe-platform-testing || (echo "❌ kpqe-platform-testing folder missing" && exit 1)
          test -d release-dashboard || (echo "❌ release-dashboard folder missing" && exit 1)
          test -d release-decision || (echo "❌ release-decision folder missing" && exit 1)

          echo "===== Checking required files ====="
          test -f application-testing/pom.xml || (echo "❌ application-testing/pom.xml missing" && exit 1)
          test -f security-testing/run-gitleaks.sh || (echo "❌ run-gitleaks.sh missing" && exit 1)
          test -f security-testing/run-semgrep.sh || (echo "❌ run-semgrep.sh missing" && exit 1)
          test -f security-testing/run-trivy.sh || (echo "❌ run-trivy.sh missing" && exit 1)
          test -f release-dashboard/generate-dashboard.py || (echo "❌ release-dashboard/generate-dashboard.py missing" && exit 1)
          test -f release-decision/final-decision.py || (echo "❌ release-decision/final-decision.py missing" && exit 1)

          echo "✅ Stage 1 validation passed"

      - name: Setup Java 21
        uses: actions/setup-java@v4
        with:
          distribution: "temurin"
          java-version: "21"
          cache: "maven"

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Chrome dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            libnss3 \
            libxss1 \
            libasound2t64 \
            libatk-bridge2.0-0 \
            libgtk-3-0 \
            libgbm1 \
            fonts-liberation \
            xdg-utils \
            unzip \
            curl

      - name: Setup Chrome
        uses: browser-actions/setup-chrome@v1

      - name: Debug Chrome + System Versions
        run: |
          echo "===== Chrome Version ====="
          google-chrome --version || true

          echo "===== Chromedriver Version ====="
          chromedriver --version || true

          echo "===== Java Version ====="
          java -version || true

          echo "===== Maven Version ====="
          mvn -v || true

          echo "===== Python Version ====="
          python --version || true

      - name: Clone Target Big Mart Repo
        run: |
          git clone https://github.com/asutosh-2000/Big-Mart-Sales-Prediction.git target-app
          echo "===== Target Repo Files ====="
          ls -la target-app

      - name: Install Flask App Dependencies
        run: |
          cd target-app
          python -m pip install --upgrade pip
          pip install flask joblib pandas numpy scikit-learn xgboost

      - name: Start Flask App
        run: |
          cd target-app
          export FLASK_ENV=production
          export PYTHONUNBUFFERED=1

          nohup python app.py > flask.log 2>&1 &
          echo $! > flask.pid

          echo "Flask PID: $(cat flask.pid)"
          sleep 6

          echo "===== Flask Log (After Start) ====="
          cat flask.log || true

          echo "===== Ports Listening ====="
          ss -ltnp || true

      - name: Verify Flask App
        run: |
          echo "===== Checking Flask Health ====="

          for i in {1..40}; do
            echo "Checking Flask... attempt $i"

            if curl -s -o /dev/null -w "%{http_code}" http://127.0.0.1:9457 | grep -q "200"; then
              echo "✅ Flask is UP on port 9457"
              exit 0
            fi

            sleep 2
          done

          echo "❌ Flask did not start properly"
          cat target-app/flask.log || true
          exit 1

      - name: Run Automation Tests
        run: |
          cd application-testing
          mvn clean test -Dheadless=true

      - name: Generate Allure Report (CLI Mode)
        if: always()
        run: |
          echo "===== Installing Allure CLI ====="
          curl -Ls https://github.com/allure-framework/allure2/releases/download/2.27.0/allure-2.27.0.tgz -o allure.tgz
          tar -xzf allure.tgz
          sudo mv allure-2.27.0 /opt/allure
          sudo ln -sf /opt/allure/bin/allure /usr/local/bin/allure

          echo "===== Generating Allure Report ====="
          cd application-testing
          mkdir -p target/site/allure-report
          allure generate target/allure-results -o target/site/allure-report --clean

      - name: Upload Allure Report as Artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: allure-report
          path: application-testing/target/site/allure-report

      - name: Upload Flask Logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: flask-log
          path: target-app/flask.log

      - name: Deploy Allure Report to GitHub Pages
        if: always()
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: application-testing/target/site/allure-report
          publish_branch: gh-pages
          force_orphan: true



  # ============================================================
  #  LAYER 2 — Security Testing
  # ============================================================
  security-testing:
    name: Layer 2 - Security Testing (Gitleaks + Semgrep + Trivy)
    runs-on: ubuntu-latest
    needs: application-testing
    timeout-minutes: 20

    steps:
      - name: Checkout Governance Repo
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Clone Target Big Mart Repo
        run: |
          git clone https://github.com/asutosh-2000/Big-Mart-Sales-Prediction.git target-app

      - name: Make scripts executable
        run: chmod +x security-testing/*.sh

      - name: Run Gitleaks (scan target-app)
        run: ./security-testing/run-gitleaks.sh target-app || true

      - name: Run Semgrep (scan target-app)
        run: ./security-testing/run-semgrep.sh target-app || true

      - name: Run Trivy (scan target-app)
        run: ./security-testing/run-trivy.sh target-app || true

      - name: Upload Security Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: security-testing/reports/



  # ============================================================
  #  LAYER 3 — SBOM
  # ============================================================
  sbom-testing:
    name: Layer 3 - SBOM Generation + SBOM Vulnerability Scan
    runs-on: ubuntu-latest
    needs: security-testing
    timeout-minutes: 20

    steps:
      - name: Checkout Governance Repo
        uses: actions/checkout@v4

      - name: Clone Target Big Mart Repo
        run: |
          git clone https://github.com/asutosh-2000/Big-Mart-Sales-Prediction.git target-app

      - name: Make SBOM scripts executable
        run: chmod +x sbom-testing/*.sh

      - name: Generate SBOM (CycloneDX JSON)
        run: ./sbom-testing/generate-sbom.sh target-app

      - name: Scan SBOM for Vulnerabilities
        run: ./sbom-testing/scan-sbom.sh

      - name: Upload SBOM Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: sbom-reports
          path: sbom-testing/reports/



  # ============================================================
  #  LAYER 4 — KPQE Platform Testing (KIND + pytest)
  # ============================================================
  kpqe-platform-testing:
    name: Layer 4 - Kubernetes Readiness + KPQE Platform Testing
    runs-on: ubuntu-latest
    needs: sbom-testing
    timeout-minutes: 25

    steps:
      - name: Checkout Governance Repo
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install kubectl
        run: |
          curl -LO "https://dl.k8s.io/release/$(curl -Ls https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
          chmod +x kubectl
          sudo mv kubectl /usr/local/bin/kubectl
          kubectl version --client

      - name: Setup KIND cluster
        uses: helm/kind-action@v1.10.0
        with:
          cluster_name: kpqe-cluster
          wait: 120s

      # ✅ GUARANTEED FIX: kubeconfig + context
      - name: Force kubeconfig + context
        run: |
          echo "===== Checking kubeconfig ====="
          ls -la $HOME/.kube || true
          ls -la $HOME/.kube/config || true

          if [ ! -f "$HOME/.kube/config" ]; then
            echo "❌ kubeconfig not found at $HOME/.kube/config"
            exit 1
          fi

          echo "KUBECONFIG=$HOME/.kube/config" >> $GITHUB_ENV
          export KUBECONFIG=$HOME/.kube/config

          kubectl config get-contexts
          kubectl config use-context kind-kpqe-cluster
          kubectl config current-context

      - name: Wait for Kubernetes API Ready
        run: |
          export KUBECONFIG=$HOME/.kube/config
          kubectl config use-context kind-kpqe-cluster

          for i in {1..40}; do
            echo "Attempt $i: Checking cluster..."
            kubectl get nodes && exit 0
            sleep 3
          done

          echo "❌ Cluster API not responding"
          exit 1

      - name: Verify Kubernetes Cluster
        run: |
          export KUBECONFIG=$HOME/.kube/config
          kubectl config use-context kind-kpqe-cluster
          kubectl cluster-info
          kubectl get nodes -o wide
          kubectl get namespaces

      - name: Install KPQE Python requirements
        run: |
          cd kpqe-platform-testing
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Apply KPQE Namespace + RBAC
        run: |
          export KUBECONFIG=$HOME/.kube/config
          kubectl config use-context kind-kpqe-cluster
          kubectl apply -f kpqe-platform-testing/k8s/namespace.yaml
          kubectl apply -f kpqe-platform-testing/k8s/kpqe-rbac.yaml

      - name: Run KPQE Platform Tests
        run: |
          cd kpqe-platform-testing
          pytest -q platform-tests --disable-warnings --maxfail=1

      - name: Evaluate KPQE Quality Gate
        if: always()
        run: |
          cd kpqe-platform-testing
          python -m quality_gates.evaluate_platform > kpqe-release-decision.txt
          cat kpqe-release-decision.txt

      - name: Export Cluster + Pod Health (JSON)
        if: always()
        run: |
          export KUBECONFIG=$HOME/.kube/config
          kubectl config use-context kind-kpqe-cluster

          mkdir -p kpqe-platform-testing/reports
          kubectl get nodes -o json > kpqe-platform-testing/reports/nodes.json
          kubectl get pods -A -o json > kpqe-platform-testing/reports/pods.json
          kubectl get ns -o json > kpqe-platform-testing/reports/namespaces.json

      - name: Upload KPQE Reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: kpqe-platform-reports
          path: |
            kpqe-platform-testing/kpqe-release-decision.txt
            kpqe-platform-testing/reports/


  # ============================================================
  #  LAYER 5 — Release Dashboard
  # ============================================================
  release-dashboard:
    name: Layer 5 - Single Consolidated Release Dashboard
    runs-on: ubuntu-latest
    needs: kpqe-platform-testing
    timeout-minutes: 10

    steps:
      - name: Checkout Governance Repo
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dashboard requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r release-dashboard/requirements.txt

      - name: Download all workflow artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Debug - list downloaded artifacts
        run: |
          echo "===== ARTIFACTS DOWNLOADED ====="
          ls -R artifacts || true

      - name: Prepare dashboard inputs
        run: |
          mkdir -p security-testing/reports
          mkdir -p sbom-testing/reports
          mkdir -p kpqe-platform-testing/reports
          mkdir -p application-testing/target/site/allure-report

          cp -r artifacts/allure-report/* application-testing/target/site/allure-report/ || true
          cp -r artifacts/security-reports/* security-testing/reports/ || true
          cp -r artifacts/sbom-reports/* sbom-testing/reports/ || true
          cp -r artifacts/kpqe-platform-reports/* kpqe-platform-testing/ || true

          echo "===== INPUTS READY ====="
          ls -la security-testing/reports || true
          ls -la sbom-testing/reports || true
          ls -la kpqe-platform-testing/reports || true
          ls -la application-testing/target/site/allure-report || true

      - name: Generate dashboard
        run: |
          python release-dashboard/generate-dashboard.py

      # ✅ GUARANTEED FIX: Normalize output folder
      - name: Normalize dashboard output (GUARANTEED)
        run: |
          mkdir -p release-dashboard/output

          echo "===== Searching for dashboard files ====="
          find release-dashboard -maxdepth 3 -type f || true

          # Copy index.html (root -> output)
          if [ -f "release-dashboard/index.html" ]; then
            cp release-dashboard/index.html release-dashboard/output/index.html
            echo "✅ Copied index.html -> output/"
          fi

          # Copy release-summary.json (root -> output)
          if [ -f "release-dashboard/release-summary.json" ]; then
            cp release-dashboard/release-summary.json release-dashboard/output/release-summary.json
            echo "✅ Copied release-summary.json -> output/"
          fi

          # Final check
          if [ ! -f "release-dashboard/output/index.html" ]; then
            echo "❌ index.html missing in output/"
            exit 1
          fi

          if [ ! -f "release-dashboard/output/release-summary.json" ]; then
            echo "❌ release-summary.json missing in output/"
            exit 1
          fi

          echo "===== FINAL OUTPUT READY ====="
          ls -la release-dashboard/output

      - name: Upload Release Dashboard Artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: release-dashboard
          path: release-dashboard/output/


  # ============================================================
  #  LAYER 6 — Final Release Decision Engine
  # ============================================================
  final-release-decision:
    name: Layer 6 - Final Release Decision Engine
    runs-on: ubuntu-latest
    needs: release-dashboard
    timeout-minutes: 10

    steps:
      - name: Checkout Governance Repo
        uses: actions/checkout@v4

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install decision engine requirements
        run: |
          python -m pip install --upgrade pip
          pip install -r release-decision/requirements.txt

      - name: Download Release Dashboard Artifact
        uses: actions/download-artifact@v4
        with:
          name: release-dashboard
          path: release-dashboard-artifact

      - name: Debug - check downloaded artifact
        run: |
          echo "===== release-dashboard-artifact ====="
          ls -R release-dashboard-artifact || true

      # ✅ GUARANTEED FIX: Copy release-summary.json correctly
      - name: Copy release-summary.json into decision engine input
        run: |
          mkdir -p release-decision/input

          if [ -f "release-dashboard-artifact/release-summary.json" ]; then
            cp release-dashboard-artifact/release-summary.json release-decision/input/release-summary.json
          elif [ -f "release-dashboard-artifact/output/release-summary.json" ]; then
            cp release-dashboard-artifact/output/release-summary.json release-decision/input/release-summary.json
          else
            echo "❌ release-summary.json NOT FOUND in artifact"
            find release-dashboard-artifact -type f || true
            exit 1
          fi

          echo "✅ Copied release-summary.json successfully"
          ls -la release-decision/input

      - name: Run Final Release Decision Engine
        run: |
          python release-decision/final-decision.py

      - name: Upload Final Decision Artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: final-release-decision
          path: release-decision/output/
